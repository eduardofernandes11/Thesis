{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jMpL5iTPyYG"
      },
      "outputs": [],
      "source": [
        "%pip install transformers\n",
        "%pip install scikit-learn\n",
        "%pip install modAL\n",
        "%pip install datasets\n",
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qF_UGEOpPrDb"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nGVnKenzjvh",
        "outputId": "447b32c1-b480-4fab-b2c2-1caf5f749400"
      },
      "outputs": [],
      "source": [
        "# Define the mapping from emotion names to numbers\n",
        "emotion_mapping = {\n",
        "    'admiration': 1, 'amusement': 2, 'anger': 3, 'annoyance': 4,\n",
        "    'approval': 5, 'caring': 6, 'confusion': 7, 'curiosity': 8,\n",
        "    'desire': 9, 'disappointment': 10, 'disapproval': 11, 'disgust': 12,\n",
        "    'embarrassment': 13, 'excitement': 14, 'fear': 15, 'gratitude': 16,\n",
        "    'grief': 17, 'joy': 18, 'love': 19, 'nervousness': 20, 'optimism': 21,\n",
        "    'pride': 22, 'realization': 23, 'relief': 24, 'remorse': 25, 'sadness': 26,\n",
        "    'surprise': 27, 'neutral': 28\n",
        "}\n",
        "\n",
        "def process_csv(file_path):\n",
        "    \"\"\"Processes a single CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path, delimiter=',')\n",
        "\n",
        "    # Find emotion numbers (same as your existing code)\n",
        "    emotion_numbers = []\n",
        "    for index, row in df.iterrows():\n",
        "        emotion_number = 0\n",
        "        for emotion, number in emotion_mapping.items():\n",
        "            if row['emotion'] == emotion:\n",
        "                emotion_number = number\n",
        "                break\n",
        "        emotion_numbers.append(emotion_number)\n",
        "\n",
        "    # Add emotion numbers as a new column\n",
        "    df['emotion'] = emotion_numbers\n",
        "\n",
        "    # Select relevant columns and ensure data types\n",
        "    new_df = df[['text', 'emotion']]\n",
        "    new_df['text'] = new_df['text'].astype(str)\n",
        "    new_df['emotion'] = new_df['emotion'].astype(int)\n",
        "\n",
        "    return new_df\n",
        "\n",
        "initial_labeled_data = process_csv('active_learning_emotions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "initial_labeled_data.shape\n",
        "print(initial_labeled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enron_dataset = load_dataset('SetFit/enron_spam')\n",
        "data = enron_dataset['train']  # Access the 'train' split of the dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Select relevant columns and ensure data types\n",
        "X_unlabeled = pd.DataFrame()\n",
        "X_unlabeled['text'] = df['message'].astype(str)\n",
        "X_unlabeled['emotion'] = df['label'].astype(int)\n",
        "print(X_unlabeled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "# Load pre-trained RoBERTa\n",
        "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "# model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YGJs43LlAyby"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)  # Use your model_name\n",
        "\n",
        "# def tokenize_function(examples):\n",
        "#     return tokenizer(examples, padding='max_length', truncation=True)\n",
        "\n",
        "# tokenized_dataset = initial_labeled_data['text'].map(tokenize_function)\n",
        "\n",
        "# Updated logic for accessing tokenized data (assuming PyTorch)\n",
        "# X_train = [encoding['input_ids'] for encoding in tokenized_dataset]\n",
        "# y_train = [encoding['input_ids'] for encoding in initial_labeled_data['emotion'].map(tokenize_function)]\n",
        "\n",
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.emotion\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = initial_labeled_data.reset_index(drop=True)\n",
        "test_data = X_unlabeled.reset_index(drop=True)\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = SentimentData(test_data[:100], tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobertaEmotionsClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaEmotionsClass, self).__init__()\n",
        "        self.l1 = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 29)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        # hidden_state = output_1[0]\n",
        "        # pooler = hidden_state[:, 0]\n",
        "        # pooler = self.pre_classifier(pooler)\n",
        "        # pooler = torch.nn.ReLU()(pooler)\n",
        "        # pooler = self.dropout(pooler)\n",
        "        # output = self.classifier(pooler)\n",
        "        return output_1.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"MODEL NAME {}\".format(model_name))\n",
        "model = RobertaEmotionsClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples\n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_new_labels_from_human(unlabeled_examples):\n",
        "    new_labels = []\n",
        "\n",
        "    print(\"EMOTIONS:\", [*emotion_mapping])\n",
        "    for example in unlabeled_examples[\"text\"]:\n",
        "        print(\"Text:\", example)\n",
        "        label_choice = input(\"Text: {}: \".format(example))\n",
        "        while label_choice not in [*emotion_mapping]:\n",
        "            print(\"Invalid input. Please enter one of the valid choices.\")\n",
        "            label_choice = input(\"Text: {}: \".format(example))\n",
        "        new_labels.append(label_choice)\n",
        "\n",
        "    return new_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def least_confidence_sampling(model, testing_loader, query_size):\n",
        "    model.eval()\n",
        "    all_confidences = torch.tensor([], device=device)  # Initialize an empty tensor\n",
        "\n",
        "    with torch.no_grad():  # Temporarily disable gradient calculations\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            confidence = torch.max(outputs.softmax(dim=1), dim=1)[0]  # Extract confidence score\n",
        "            all_confidences = torch.cat((all_confidences, confidence), dim=0)  # Concatenate to tensor\n",
        "\n",
        "    query_idx = torch.argsort(all_confidences)[:query_size]  # Select indices of 'query_size' least confident examples           \n",
        "    return query_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 2\n",
        "query_size = 1  # number of instances to query per iteration\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "\n",
        "    # Query strategy: Uncertainty Sampling\n",
        "    query_idx = least_confidence_sampling(model, testing_loader, query_size) \n",
        "\n",
        "    # Convert to NumPy array and move to CPU\n",
        "    query_idx_numpy = query_idx.cpu().numpy() \n",
        "\n",
        "    # Get labels from a human annotator for the selected instances\n",
        "    new_labels = get_new_labels_from_human(test_data.loc[query_idx_numpy])\n",
        "    print(new_labels)\n",
        "\n",
        "    # Update datasets (assuming new_labels is a list or array of equal length to query_size)\n",
        "    train_data = pd.concat([train_data, test_data.iloc[query_idx_numpy]], ignore_index=True)\n",
        "    test_data = test_data.drop(query_idx_numpy).reset_index(drop=True)\n",
        "\n",
        "    # Update DataLoaders\n",
        "    training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "    testing_set = SentimentData(test_data[:100], tokenizer, MAX_LEN)\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    testing_loader = DataLoader(testing_set, **test_params) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

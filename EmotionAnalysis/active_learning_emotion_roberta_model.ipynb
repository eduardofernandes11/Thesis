{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jMpL5iTPyYG"
      },
      "outputs": [],
      "source": [
        "%pip install transformers\n",
        "%pip install scikit-learn\n",
        "%pip install modAL\n",
        "%pip install datasets\n",
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF_UGEOpPrDb"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import regex as re\n",
        "from collections import defaultdict\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nGVnKenzjvh",
        "outputId": "447b32c1-b480-4fab-b2c2-1caf5f749400"
      },
      "outputs": [],
      "source": [
        "# Define the mapping from emotion names to numbers\n",
        "emotion_mapping = {\n",
        "    'admiration': 0, 'amusement': 1, 'anger': 2, 'annoyance': 3,\n",
        "    'approval': 4, 'caring': 5, 'confusion': 6, 'curiosity': 7,\n",
        "    'desire': 8, 'disappointment': 9, 'disapproval': 10, 'disgust': 11,\n",
        "    'embarrassment': 12, 'excitement': 13, 'fear': 14, 'gratitude': 15,\n",
        "    'grief': 16, 'joy': 17, 'love': 18, 'nervousness': 19, 'optimism': 20,\n",
        "    'pride': 21, 'realization': 22, 'relief': 23, 'remorse': 24, 'sadness': 25,\n",
        "    'surprise': 26, 'neutral': 27\n",
        "}\n",
        "\n",
        "def process_csv(file_path):\n",
        "    \"\"\"Processes a single CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path, delimiter=',')\n",
        "\n",
        "    # Find emotion numbers (same as your existing code)\n",
        "    emotion_numbers = []\n",
        "    for index, row in df.iterrows():\n",
        "        emotion_number = 0\n",
        "        for emotion, number in emotion_mapping.items():\n",
        "            if row['emotion'] == emotion:\n",
        "                emotion_number = number\n",
        "                break\n",
        "        emotion_numbers.append(emotion_number)\n",
        "\n",
        "    # Add emotion numbers as a new column\n",
        "    df['emotion'] = emotion_numbers\n",
        "\n",
        "    # Select relevant columns and ensure data types\n",
        "    new_df = df[['text', 'emotion']]\n",
        "    new_df['text'] = new_df['text'].astype(str)\n",
        "    new_df['emotion'] = new_df['emotion'].astype(int)\n",
        "\n",
        "    return new_df\n",
        "\n",
        "initial_labeled_data = process_csv('active_learning_emotions.csv')\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_df, test_df = train_test_split(initial_labeled_data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print shapes to verify the split\n",
        "print(\"Training Data Shape:\", train_df.shape)\n",
        "print(\"Testing Data Shape:\", test_df.shape)\n",
        "\n",
        "print(f\"Train DF: {train_df}\")\n",
        "print(f\"Test DF: {test_df}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enron_dataset = load_dataset('SetFit/enron_spam')\n",
        "data = enron_dataset['train']  # Access the 'train' split of the dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Select relevant columns and ensure data types\n",
        "X_unlabeled = pd.DataFrame()\n",
        "X_unlabeled['text'] = df['message'].astype(str)\n",
        "X_unlabeled['emotion'] = df['label'].astype(int)\n",
        "print(X_unlabeled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis suggests some cleaning strategies:\n",
        "\n",
        "- Lowercase all text\n",
        "- Remove URLs\n",
        "- Remove all LaTeX mathematical expressions and asymptote code blocks. They may convey useful information, but for now, we donâ€™t want to be too advanced.\n",
        "- Remove erroneous LaTeX syntax and stopwords\n",
        "- Handle filters and non-alphanumeric characters\n",
        "- Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(x):\n",
        "    # lowercase all\n",
        "    # if lower:\n",
        "    #     x = x.lower()\n",
        "    \n",
        "    x = re.sub(r'http\\S+', '', x)               # remove URLs\n",
        "    x = x.replace('$$$', '$$ $')                # separate triple dollars\n",
        "    x = x.replace('\\n', ' ')                    # remove new lines\n",
        "    # x = re.sub(equation_pattern, '', x)         # remove math expressions\n",
        "    # x = re.sub(asymptote_pattern, '', x)        # remove asymptote\n",
        "    \n",
        "    # remove erroneous LaTeX syntax and stopwords\n",
        "    x = x.replace('\\\\', ' \\\\')\n",
        "    # temp = []\n",
        "    # for word in x.split():\n",
        "    #     if not word.startswith('\\\\') and not word in stopwords:\n",
        "    #         temp.append(word)\n",
        "    # x = ' '.join(temp)\n",
        "    \n",
        "    x = re.sub(r'([-;.,!?<=>])', r' \\1 ', x)    # separate filters from words\n",
        "    x = re.sub('[^A-Za-z0-9]+', ' ', x)         # remove non-alphanumeric chars\n",
        "    \n",
        "    # stemming\n",
        "    # if stem:\n",
        "    #     x = ' '.join(porter.stem(word) for word in x.split())\n",
        "    return x\n",
        "\n",
        "X_unlabeled['text'] = X_unlabeled['text'].apply(preprocess_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "# Load pre-trained RoBERTa\n",
        "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "# model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGJs43LlAyby"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)  # Use your model_name\n",
        "\n",
        "# def tokenize_function(examples):\n",
        "#     return tokenizer(examples, padding='max_length', truncation=True)\n",
        "\n",
        "# tokenized_dataset = initial_labeled_data['text'].map(tokenize_function)\n",
        "\n",
        "# Updated logic for accessing tokenized data (assuming PyTorch)\n",
        "# X_train = [encoding['input_ids'] for encoding in tokenized_dataset]\n",
        "# y_train = [encoding['input_ids'] for encoding in initial_labeled_data['emotion'].map(tokenize_function)]\n",
        "\n",
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.emotion\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = train_df.reset_index(drop=True)\n",
        "test_data = test_df.reset_index(drop=True)\n",
        "pool_data = X_unlabeled.reset_index(drop=True)\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\n",
        "pool_set = SentimentData(pool_data[:100], tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "pool_loader = DataLoader(pool_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobertaEmotionsClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaEmotionsClass, self).__init__()\n",
        "        self.l1 = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 28)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        # hidden_state = output_1[0]\n",
        "        # pooler = hidden_state[:, 0]\n",
        "        # pooler = self.pre_classifier(pooler)\n",
        "        # pooler = torch.nn.ReLU()(pooler)\n",
        "        # pooler = self.dropout(pooler)\n",
        "        # output = self.classifier(pooler)\n",
        "        return output_1.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"MODEL NAME {}\".format(model_name))\n",
        "model = RobertaEmotionsClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch, train_loader):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(train_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples\n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_wrapped(text):\n",
        "    display(HTML(f\"<pre style='word-break: break-word; white-space: pre-wrap;'>{text}</pre>\"))\n",
        "\n",
        "def get_new_labels_from_human(unlabeled_examples):\n",
        "    new_labels = []\n",
        "\n",
        "    for example in unlabeled_examples[\"text\"]:\n",
        "        print(\"EMOTIONS:\", [*emotion_mapping])\n",
        "        print_wrapped(\"Text: \" + example)\n",
        "        label_choice = input()\n",
        "        while label_choice not in [*emotion_mapping]:\n",
        "            print(\"Invalid input. Please enter one of the valid choices.\")\n",
        "            label_choice = input()\n",
        "        new_labels.append(label_choice)\n",
        "\n",
        "    return new_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def least_confidence_sampling(model, pool_loader, threshold):\n",
        "    model.eval()\n",
        "    all_confidences = torch.tensor([], device=device)  # Initialize an empty tensor\n",
        "    #query_idx = []  # Initialize an empty list for indices\n",
        "\n",
        "    with torch.no_grad():  # Temporarily disable gradient calculations\n",
        "        for _, data in tqdm(enumerate(pool_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            confidence = torch.max(outputs.softmax(dim=1), dim=1)[0]  # Extract confidence score\n",
        "            all_confidences = torch.cat((all_confidences, confidence), dim=0)  # Concatenate to tensor\n",
        "\n",
        "    query_idx = torch.where(all_confidences < threshold)[0]  # Select indices of 'query_size' least confident examples       \n",
        "\n",
        "    return query_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_score(model, testing_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []  # Initialize an empty list\n",
        "    y_test = []  # Initialize an empty list to store true labels\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            predictions = outputs.argmax(dim=1).cpu().numpy()  # Get predicted class labels\n",
        "            all_predictions.extend(predictions)  # Add predictions to the list\n",
        "            targets = data['targets'].numpy()  # Extract targets and convert to NumPy array\n",
        "            y_test.extend(targets)  # Append to the list of true labels  \n",
        "\n",
        "    metrics = precision_recall_fscore_support(y_test, all_predictions, average='weighted')\n",
        "    performance = {\n",
        "        'precision': '{:0.2f}'.format(metrics[0]),\n",
        "        'recall': '{:0.2f}'.format(metrics[1]),\n",
        "        'f1': '{:0.2f}'.format(metrics[2])\n",
        "    }\n",
        "    return performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "def graph_plot(iteration, score, scores):\n",
        "    #clear_output()\n",
        "    fig, ax = plt.subplots(3, 1, sharex=True, figsize=(10,10))\n",
        "    fig.suptitle('Active Learning Scores')\n",
        "    for j, (key, value) in enumerate(score.items()):\n",
        "        scores[key].append(value)\n",
        "        ax[j].plot(range(iteration+1), scores[key])\n",
        "        ax[j].set_title(key)\n",
        "    plt.xlabel('Number of query')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 2\n",
        "num_iterations = 3\n",
        "threshold = 0.3\n",
        "\n",
        "scores = {\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'f1': []  \n",
        "}\n",
        "\n",
        "train_loader = training_loader\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    for epoch in range(EPOCHS):\n",
        "        train(epoch, train_loader)\n",
        "\n",
        "    # Calculate score\n",
        "    score = calc_score(model, testing_loader)\n",
        "\n",
        "    # Graph plot\n",
        "    graph_plot(iteration, score, scores)\n",
        "\n",
        "    # Query strategy: Uncertainty Sampling\n",
        "    query_idx = least_confidence_sampling(model, pool_loader, threshold) \n",
        "\n",
        "    if len(query_idx) > 0:\n",
        "        # Convert to NumPy array and move to CPU\n",
        "        query_idx_numpy = query_idx.cpu().numpy() \n",
        "\n",
        "        # Get labels from a human annotator for the selected instances\n",
        "        new_labels = get_new_labels_from_human(pool_data.loc[query_idx_numpy])\n",
        "\n",
        "        # Update labels in pool_data\n",
        "        pool_data.loc[query_idx_numpy, 'emotion'] = [emotion_mapping[label] for label in new_labels]\n",
        "\n",
        "        # Update datasets \n",
        "        try:\n",
        "            train_data = pd.concat([train_data, pool_data.iloc[query_idx_numpy]], ignore_index=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered: {e}\")\n",
        "        pool_data = pool_data.drop(query_idx_numpy).reset_index(drop=True)\n",
        "        \n",
        "        # Update DataLoaders\n",
        "        train_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "        pool_set = SentimentData(pool_data[:100], tokenizer, MAX_LEN)\n",
        "        train_loader = DataLoader(train_set, **train_params)\n",
        "        pool_loader = DataLoader(pool_set, **test_params) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_file = 'pytorch_roberta_emotion_email.bin'\n",
        "output_vocab_file = './'\n",
        "\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('All files saved')\n",
        "print('This tutorial is completed')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
